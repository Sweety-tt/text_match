{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import (DataLoader,RandomSampler,SequentialSampler,TensorDataset)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "import Levenshtein\n",
    "from pypinyin import lazy_pinyin\n",
    "import jieba\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "###参数设置\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--random_seed',type=int,help='随机种子',default = 78)\n",
    "parser.add_argument('--file_name',type=str,default = 'baseline1')\n",
    "parser.add_argument('--file_path',type=str,default='./log/log_modeltest.txt')\n",
    "parser.add_argument('--max_seq_length',type=int,default = 60)\n",
    "parser.add_argument('--learning_rate',type=float,default = 5e-5)\n",
    "parser.add_argument('--num_epochs',type=int,default = 8)\n",
    "parser.add_argument('--batch_size',type=int,default = 100)\n",
    "parser.add_argument('--patience',type=int,default = 5)\n",
    "parser.add_argument('--train_input',type=str,default = './data/train/')\n",
    "parser.add_argument('--test_input',type=str,default = './data/test/')\n",
    "parser.add_argument('--model_name_or_path',type=str,default = './pretrain_models/ernie-gram/')\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED']= str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(args.random_seed)\n",
    "\n",
    "def get_logger(filename,verbosity=1,name=None):\n",
    "    level_dict = {0:logging.DEBUG,1:logging.INFO,2:logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s]## %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "    timestamp = time.strftime(\"%Y.%m.%d_%H.%M.%S\",time.localtime())\n",
    "    \n",
    "    fh = logging.FileHandler(filename,\"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "    \n",
    "    return logger\n",
    "logger = get_logger(args.file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self,s1,s2,label=None):\n",
    "        self.s1 = s1\n",
    "        self.s2 = s2\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,choices_features,label):\n",
    "        _,input_ids,input_mask,segment_ids = choices_features[0]\n",
    "        self.choices_features = {\n",
    "            'input_ids':input_ids,\n",
    "            'input_mask':input_mask,\n",
    "            'segment_ids':segment_ids\n",
    "        }\n",
    "        self.label = label\n",
    "        \n",
    "def read_data(file_name):\n",
    "    examples = []\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                line =line.split('\\t')\n",
    "                examples.append(InputExample(s1=line[0],s2=line[1],label=line[2]) if len(line)==3 else None)\n",
    "    return examples\n",
    "\n",
    "def read_examples(dir,split='train'):\n",
    "    examples = []\n",
    "    for path in os.listdir(dir):\n",
    "        if split=='train':\n",
    "            for file_name in os.listdir(dir+path):\n",
    "                example = read_data(os.path.join(dir+path,file_name))\n",
    "                examples.extend(example)\n",
    "    return examples\n",
    "\n",
    "def _truncate_seq_pair(tokens_a,tokens_b,max_length):\n",
    "    while True:\n",
    "        total_length = len(tokens_a)+len(tokens_b)\n",
    "        if total_length<=max_length:\n",
    "            break\n",
    "        if len(tokens_a)>len(tokens_b):\n",
    "            tokens_a.pop()    \n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def convert_examples_to_features(examples,tokenizer,max_seq_length,is_training):\n",
    "    features = []\n",
    "    for example_index,example in enumerate(examples):\n",
    "        \n",
    "        s1 = tokenizer.tokenize(example.s1)\n",
    "        s2 = tokenizer.tokenize(example.s2)\n",
    "        _truncate_seq_pair(s1, s2, max_seq_length)\n",
    "        \n",
    "        choices_features = []\n",
    "        \n",
    "        tokens = [\"[CLS]\"] + s1 +[\"[SEP]\"] + s2 + [\"[SEP]\"]\n",
    "        segment_ids = [0]*(len(s1)+2)+[1]*(len(s2)+1)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1]*len(tokens)\n",
    "        ## mask has 1 for real token 0 for padding token\n",
    "        \n",
    "        padding_length = max_seq_length-len(tokens)+3\n",
    "        input_ids +=([0]*padding_length)\n",
    "        input_mask +=([0]*padding_length)\n",
    "        segment_ids +=([0]*padding_length)\n",
    "        choices_features.append((tokens,input_ids,input_mask,segment_ids))\n",
    "        \n",
    "        label = example.label\n",
    "        if example_index < 1 and is_training:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"idx:{}\".format(example_index))\n",
    "            logger.info(\"tokens:{}\".format(' '.join(tokens).replace('\\u2581','_')))\n",
    "            logger.info(\"input_ids:{}\".format(' '.join(map(str,input_ids))))\n",
    "            logger.info(\"input_mask:{}\".format(len(input_mask)))\n",
    "            logger.info(\"segment_ids:{}\".format(len(segment_ids)))\n",
    "            logger.info(\"label:{}\".format(label))\n",
    "        \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                choices_features = choices_features,\n",
    "                label = label\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return features\n",
    "    \n",
    "def select_field(features,field):\n",
    "    return[\n",
    "        feature.choices_features[field] for feature in features\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,model_name_or_path,hidden_size=768,num_class=2):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        \n",
    "        self.config = BertConfig.from_pretrained(model_name_or_path,num_lables=num_class)\n",
    "        self.config.output_hidden_states =True\n",
    "        self.bert = BertModel.from_pretrained(model_name_or_path,config=self.config)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True    #如果为False ,则冻结预训练模型的权重\n",
    "        self.weights = nn.Parameter(torch.rand(13,1))\n",
    "        self.fc = nn.Linear(hidden_size*2,num_class)\n",
    "        self.dropouts = nn.ModuleList([\n",
    "            nn.Dropout(0.2) for _ in range(5)\n",
    "        ])\n",
    "    \n",
    "    def forward(self,input_ids,input_mask,segment_ids,y=None,loss_fn=None):\n",
    "        output = self.bert(input_ids,token_type_ids=segment_ids,attention_mask=input_mask)\n",
    "        last_hidden = output.last_hidden_state # 最后一层输出的隐藏状态 batch_size * sequence_length * hidden_size\n",
    "        all_hidden_states = output.hidden_states \n",
    "        batch_size = input_ids.shape[0]\n",
    "        #####补充   output 为tuple类型，共四个部分，last_hidden_state ，pooler_output （序列第一个token的最后一层隐藏状态），\n",
    "        ######      hiddden_states 是一个元组（第一个元素是embedding，其余元素是各层的输出）\n",
    "        ######      attentions 元素是每一层的注意力权重，用于计算self.attention heads 的加权平均值\n",
    "        \n",
    "        ht_cls = torch.cat(all_hidden_states)[:,:1,:].view(13,batch_size,1,768)  # 每个样本的每一层的ht_cls\n",
    "        atten = torch.sum(ht_cls*self.weights.view(13,1,1,1),dim=[1,3])#缩减维度\n",
    "        atten = F.softmax(atten.view(-1),dim=0)\n",
    "        feature = torch.sum(ht_cls*atten.view(13,1,1,1),dim=[0,2])\n",
    "        f = torch.mean(last_hidden,1)\n",
    "        feature = torch.cat((feature,f),1)  #batch_size*(hidden_size*2)\n",
    "        \n",
    "        for i , dropout in enumerate(self.dropouts):\n",
    "            if i==0:\n",
    "                h = self.fc(dropout(feature))\n",
    "                if loss_fn is not None:\n",
    "                    loss = loss_fn(h,y)\n",
    "            else:\n",
    "                hi = self.fc(dropout(feature))\n",
    "                h = h + hi\n",
    "                if loss_fn is not None:\n",
    "                    loss += loss_fn(hi,y)\n",
    "        \n",
    "        if loss_fn is not None:\n",
    "            return h/len(self.dropouts), loss/len(self.dropouts)\n",
    "        return h/len(self,dropouts)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "    def __init__(self,params,lr=1e-3,betas=(0.9,0.999),eps=1e-8,weight_decay=0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                            N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class focal_loss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, num_classes = 2, size_average=True):\n",
    "        \"\"\"\n",
    "        focal_loss损失函数, -α(1-yi)**γ *ce_loss(xi,yi)\n",
    "        步骤详细的实现了 focal_loss损失函数.\n",
    "        :param alpha:   阿尔法α,类别权重.      当α是列表时,为各类别权重,当α为常数时,类别权重为[α, 1-α, 1-α, ....],常用于 目标检测算法中抑制背景类 , retainnet中设置为0.25\n",
    "        :param gamma:   伽马γ,难易样本调节参数. retainnet中设置为2\n",
    "        :param num_classes:     类别数量\n",
    "        :param size_average:    损失计算方式,默认取均值\n",
    "        \"\"\"\n",
    "        super(focal_loss,self).__init__()\n",
    "        self.size_average = size_average\n",
    "        if isinstance(alpha,list):\n",
    "            assert len(alpha)==num_classes   # α可以以list方式输入,size:[num_classes] 用于对不同类别精细地赋予权重\n",
    "            # print(\" --- Focal_loss alpha = {}, 将对每一类权重进行精细化赋值 --- \".format(alpha))\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        else:\n",
    "            assert alpha<1   #如果α为一个常数,则降低第一类的影响,在目标检测中为第一类\n",
    "            # print(\" --- Focal_loss alpha = {} ,将对背景类进行衰减,请在目标检测任务中使用 --- \".format(alpha))\n",
    "            self.alpha = torch.zeros(num_classes)\n",
    "            self.alpha[0] += alpha\n",
    "            self.alpha[1:] += (1-alpha) # α 最终为 [ α, 1-α, 1-α, 1-α, 1-α, ...] size:[num_classes]\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        \"\"\"\n",
    "        focal_loss损失计算\n",
    "        :param preds:   预测类别. size:[B,N,C] or [B,C]    分别对应与检测与分类任务, B 批次, N检测框数, C类别数\n",
    "        :param labels:  实际类别. size:[B,N] or [B]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # assert preds.dim()==2 and labels.dim()==1\n",
    "        preds = preds.view(-1,preds.size(-1))\n",
    "        self.alpha = self.alpha.to(preds.device)\n",
    "        preds_logsoft = F.log_softmax(preds, dim=1) # log_softmax\n",
    "        preds_softmax = torch.exp(preds_logsoft)    # softmax\n",
    "\n",
    "        preds_softmax = preds_softmax.gather(1,labels.view(-1,1))   # 这部分实现nll_loss ( crossempty = log_softmax + nll )\n",
    "        preds_logsoft = preds_logsoft.gather(1,labels.view(-1,1))\n",
    "        self.alpha = self.alpha.gather(0,labels.view(-1))\n",
    "        loss = -torch.mul(torch.pow((1-preds_softmax), self.gamma), preds_logsoft)  # torch.pow((1-preds_softmax), self.gamma) 为focal loss中 (1-pt)**γ\n",
    "\n",
    "      #  loss = torch.mul(self.alpha, loss.t())\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='bert.embeddings.'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='bert.embeddings.'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "\n",
    "class PGD():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.emb_backup = {}\n",
    "        self.grad_backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., alpha=0.3, emb_name='bert.embeddings.', is_first_attack=False):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                if is_first_attack:\n",
    "                    self.emb_backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = alpha * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = self.project(name, param.data, epsilon)\n",
    "\n",
    "    def restore(self, emb_name='bert.embeddings.'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name: \n",
    "                assert name in self.emb_backup\n",
    "                param.data = self.emb_backup[name]\n",
    "        self.emb_backup = {}\n",
    "\n",
    "    def project(self, param_name, param_data, epsilon):\n",
    "        r = param_data - self.emb_backup[param_name]\n",
    "        if torch.norm(r) > epsilon:\n",
    "            r = epsilon * r / torch.norm(r)\n",
    "        return self.emb_backup[param_name] + r\n",
    "\n",
    "    def backup_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # 不对最后的 bert.pooler 层和 linear1 层做对抗训练\n",
    "                if 'encoder' in name or 'bert.embeddings.' in name:\n",
    "                    self.grad_backup[name] = param.grad.clone()\n",
    "\n",
    "    def restore_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'encoder' in name or 'bert.embeddings.' in name:\n",
    "                    param.grad = self.grad_backup[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return acc, f1\n",
    "\n",
    "def set_lr(optimizer, value):\n",
    "    for p in optimizer.param_groups:\n",
    "        p['lr'] = value\n",
    "\n",
    "#指数加权平均\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-22 10:52:13,993][2469872708.py][line:70][INFO]## *** Example ***\n",
      "[2021-11-22 10:52:13,994][2469872708.py][line:71][INFO]## idx:0\n",
      "[2021-11-22 10:52:13,994][2469872708.py][line:72][INFO]## tokens:[CLS] 喜 欢 打 篮 球 的 男 生 喜 欢 什 么 样 的 女 生 [SEP] 爱 打 篮 球 的 男 生 喜 欢 什 么 样 的 女 生 [SEP]\n",
      "[2021-11-22 10:52:13,995][2469872708.py][line:73][INFO]## input_ids:1 692 811 445 2001 497 5 654 21 692 811 614 356 314 5 291 21 2 329 445 2001 497 5 654 21 692 811 614 356 314 5 291 21 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[2021-11-22 10:52:13,996][2469872708.py][line:74][INFO]## input_mask:63\n",
      "[2021-11-22 10:52:13,996][2469872708.py][line:75][INFO]## segment_ids:63\n",
      "[2021-11-22 10:52:13,997][2469872708.py][line:76][INFO]## label:1\n",
      "[2021-11-22 10:54:12,337][1912096065.py][line:8][INFO]## shape: (698577, 63)\n",
      "[2021-11-22 10:54:18,288][1912096065.py][line:12][INFO]## Counter({0: 355865, 1: 342712})\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path, do_lower_case=True)\n",
    "train_examples = read_examples(args.train_input, split='train')\n",
    "train_features = convert_examples_to_features(\n",
    "    train_examples, tokenizer, args.max_seq_length,True)\n",
    "\n",
    "all_input_ids = np.array(select_field(train_features, 'input_ids'))\n",
    "logger.info('shape: {}'.format(all_input_ids.shape))\n",
    "all_input_mask = np.array(select_field(train_features, 'input_mask'))\n",
    "all_segment_ids = np.array(select_field(train_features, 'segment_ids'))\n",
    "all_label = np.array([int(f.label) for f in train_features])\n",
    "logger.info(Counter(all_label))\n",
    "\n",
    "\n",
    "test_examples = read_examples(args.test_input, split='test')\n",
    "test_features = convert_examples_to_features(\n",
    "    test_examples, tokenizer, args.max_seq_length, True)\n",
    "test_input_ids = torch.tensor(select_field(test_features, 'input_ids'), dtype=torch.long)\n",
    "test_input_mask = torch.tensor(select_field(test_features, 'input_mask'), dtype=torch.long)\n",
    "test_segment_ids = torch.tensor(select_field(test_features, 'segment_ids'), dtype=torch.long)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=args.random_seed)\n",
    "oof_train = np.zeros((len(train_examples), 2), dtype=np.float32)\n",
    "oof_test = np.zeros((len(test_examples), 2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-22 12:56:57,010][2529106677.py][line:3][INFO]## ================     fold 0        ===============\n",
      " 13%|█████████████▎                                                                                     | 752/5589 [01:58<12:35,  6.40it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = args.batch_size\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(all_label, all_label)):\n",
    "    logger.info('================     fold {}        ==============='.format(fold))\n",
    "\n",
    "    # 处理模型输入数据\n",
    "    train_input_ids = torch.tensor(all_input_ids[train_index], dtype=torch.long)\n",
    "    train_input_mask = torch.tensor(all_input_mask[train_index], dtype=torch.long)\n",
    "    train_segment_ids = torch.tensor(all_segment_ids[train_index], dtype=torch.long)\n",
    "    train_label = torch.tensor(all_label[train_index], dtype=torch.long)\n",
    "\n",
    "    valid_input_ids = torch.tensor(all_input_ids[valid_index], dtype=torch.long)\n",
    "    valid_input_mask = torch.tensor(all_input_mask[valid_index], dtype=torch.long)\n",
    "    valid_segment_ids = torch.tensor(all_segment_ids[valid_index], dtype=torch.long)\n",
    "    valid_label = torch.tensor(all_label[valid_index], dtype=torch.long)\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\n",
    "    valid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\n",
    "    test = torch.utils.data.TensorDataset(test_input_ids, test_input_mask, test_segment_ids)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=args.batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    model = NeuralNet(args.model_name_or_path).cuda()\n",
    "    model.cuda()\n",
    "    # model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#     loss_fn = focal_loss()\n",
    "    \n",
    "#     fgm = FGM(model)\n",
    "    \n",
    "\n",
    "\n",
    "    # 优化器定义\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "\n",
    "    optimizer = RAdam(optimizer_grouped_parameters, lr=args.learning_rate, eps=1e-6)\n",
    "    total_steps = args.num_epochs * len(train_loader)\n",
    "    # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader)*2, num_training_steps=total_steps)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_f1 = 0.\n",
    "    valid_best = np.zeros((valid_label.size(0), 2))\n",
    "\n",
    "    early_stop = 0\n",
    "    ema = EMA(model, 0.999)\n",
    "    ema.register()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        train_loss = 0.\n",
    "        lr_list = []\n",
    "        # if epoch > 2:\n",
    "        #     set_lr(optimizer, 2e-5)\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            x_ids, x_mask, x_sids, y_truth = batch\n",
    "            with autocast():\n",
    "                y_pred, loss = model(x_ids, x_mask, x_sids, y=y_truth, loss_fn=loss_fn)\n",
    "                        \n",
    "            scaler.scale(loss.mean()).backward()\n",
    "            #加入对抗训练\n",
    "#             fgm.attack()\n",
    "#             y_pred, loss_sum = model(x_ids, x_mask, x_sids, y=y_truth, loss_fn=loss_fn)\n",
    "#             loss_sum.backward()\n",
    "#             fgm.restore()\n",
    "            scaler.step(optimizer)\n",
    "            scale = scaler.get_scale()\n",
    "            scaler.update()\n",
    "            ema.update()\n",
    "            # skip_lr_sched = (scale != scaler.get_scale())\n",
    "            # if not skip_lr_sched:\n",
    "            #     scheduler.step()\n",
    "            train_loss += loss.mean().item() / len(train_loader)\n",
    "\n",
    "        ema.apply_shadow()\n",
    "        model.eval()\n",
    "        val_loss = 0.\n",
    "        valid_preds_fold = np.zeros((valid_label.size(0), 2))\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(valid_loader)):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "                x_ids, x_mask, x_sids, y_truth = batch\n",
    "                with autocast():\n",
    "                    y_pred, loss = model(x_ids, x_mask, x_sids, y_truth, loss_fn)\n",
    "                    y_pred = y_pred.detach()\n",
    "                    val_loss += loss.mean().item() / len(valid_loader)\n",
    "                valid_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "        acc, f1 = metric(all_label[valid_index], np.argmax(valid_preds_fold, axis=1))\n",
    "        if best_f1 < f1:\n",
    "            early_stop = 0\n",
    "            best_f1 = f1\n",
    "            valid_best = valid_preds_fold\n",
    "            torch.save(model.state_dict(), './model_save/ernie_' + args.file_name + '_{}.bin'.format(fold))\n",
    "        else:\n",
    "            early_stop += 1\n",
    "        logger.info(\n",
    "            'epoch: %d, train loss: %.8f, valid loss: %.8f, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n",
    "            (epoch, train_loss, val_loss, acc, f1, best_f1))\n",
    "        torch.cuda.empty_cache()  # 每个epoch结束之后清空显存，防止显存不足\n",
    "\n",
    "        # 检测早停\n",
    "        if early_stop >= args.patience:\n",
    "            break\n",
    "\n",
    "    # 得到一折模型对测试集的预测结果\n",
    "    model.load_state_dict(torch.load('./model_save/ernie_' + args.file_name + '_{}.bin'.format(fold)))\n",
    "    test_preds_fold = np.zeros((len(test_examples), 2))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(test_loader)):\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            x_ids, x_mask, x_sids = batch\n",
    "            with autocast():\n",
    "                y_pred = model(x_ids, x_mask, x_sids).detach()\n",
    "            test_preds_fold[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "\n",
    "    oof_train[valid_index] = valid_best\n",
    "    acc, f1 = metric(all_label[valid_index], np.argmax(valid_best, axis=1))\n",
    "    logger.info('epoch: best, acc: %.8f, f1: %.8f, best_f1: %.8f\\n' %\n",
    "                (acc, f1, best_f1))\n",
    "    oof_test += test_preds_fold / 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存概率文件\n",
    "np.savetxt('./submit/train_prob/train_bert_' + args.file_name + '.txt', oof_train)\n",
    "np.savetxt('./submit/test_prob/test_bert_' + args.file_name + '.txt', oof_test)\n",
    "acc, f1 = metric(all_label, np.argmax(oof_train, axis=1))\n",
    "logger.info('epoch: best, acc: %.8f, f1: %.8f \\n' % (acc, f1))\n",
    "\n",
    "analysis = pd.DataFrame()\n",
    "analysis['s1'] = [line.s1 for line in train_examples]\n",
    "analysis['s2'] = [line.s2 for line in train_examples]\n",
    "analysis['label'] = [line.label for line in train_examples]\n",
    "analysis['pred'] = np.argmax(oof_train, axis=1).tolist()\n",
    "analysis[analysis['label'] != analysis['pred']].to_csv('analysis_{}.csv'.format(f1), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_postprocess(y_pred):\n",
    "    prior = np.array([0.6903327690476333, 0.3096672309523667]) # 训练集 oppo正负样本比例\n",
    "    \n",
    "#     prior = np.array([0.52618167,0.47381833]) \n",
    "    y_pred_uncertainty = -(y_pred * np.log(y_pred)).sum(1) / np.log(2)\n",
    "\n",
    "    threshold = 0.95\n",
    "    y_pred_confident = y_pred[y_pred_uncertainty < threshold]\n",
    "    y_pred_unconfident = y_pred[y_pred_uncertainty >= threshold]\n",
    "\n",
    "    right, alpha, iters = 0, 1, 1\n",
    "    post = []\n",
    "    for i, y in enumerate(y_pred_unconfident):\n",
    "        Y = np.concatenate([y_pred_confident, y[None]], axis=0)\n",
    "        for j in range(iters):\n",
    "            Y = Y ** alpha\n",
    "            Y /= Y.sum(axis=0, keepdims=True)\n",
    "            Y *= prior[None]\n",
    "            Y /= Y.sum(axis=1, keepdims=True)\n",
    "        y = Y[-1]\n",
    "        post.append(y.tolist())\n",
    "\n",
    "    post = np.array(post)\n",
    "    y_pred[y_pred_uncertainty >= threshold] = post\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后处理\n",
    "oof_test = prob_postprocess(oof_test)\n",
    "y_preds = np.argmax(oof_test, axis=1)\n",
    "logger.info(Counter(y_preds))\n",
    "\n",
    "with open('./output/predict_result_{}.csv'.format(f1), 'w', encoding=\"utf-8\") as f:\n",
    "    for y_pred in y_preds:\n",
    "        f.write(str(y_pred) + \"\\n\")\n",
    "\n",
    "\n",
    "def compare_pinyin(s1, s2):\n",
    "    s1_pinyin = \"\"\n",
    "    s2_pinyin = \"\"\n",
    "    for w in jieba.cut(s1):\n",
    "        s1_pinyin += ''.join(lazy_pinyin(w))\n",
    "    for w in jieba.cut(s2):\n",
    "        s2_pinyin += ''.join(lazy_pinyin(w))\n",
    "    return s1_pinyin == s2_pinyin\n",
    "\n",
    "\n",
    "def postprocess(data, pred):\n",
    "    post = []\n",
    "    for line, lable in tqdm(zip(data, pred)):\n",
    "        # r1 = correct(line.s1, line.s2)  # 339\n",
    "        r2 = compare_pinyin(line.s1, line.s2)  # 339\n",
    "        if r2:\n",
    "            post.append(1)\n",
    "        else:\n",
    "            post.append(lable)\n",
    "    post = np.array(post)\n",
    "    print(np.count_nonzero(post != pred))\n",
    "    return post\n",
    "\n",
    "post = postprocess(test_examples, y_preds)\n",
    "\n",
    "with open('./output/post_predict_result_{}.csv'.format(f1), 'w', encoding=\"utf-8\") as f:\n",
    "    for y_pred in post:\n",
    "        f.write(str(y_pred) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LinPytorch",
   "language": "python",
   "name": "linpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
